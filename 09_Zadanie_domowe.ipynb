{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 9 - Jakub KuÅ›mierski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Color printing\n",
    "from termcolor import colored\n",
    "\n",
    "#General data operations library\n",
    "import math, string, glob\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import functools\n",
    "import pandas as pd\n",
    "\n",
    "#The tensorflow library\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "import tensorflow  as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "#Plotting libraries\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Increase plots font size\n",
    "params = {'legend.fontsize': 'xx-large',\n",
    "          'figure.figsize': (10, 7),\n",
    "         'axes.labelsize': 'xx-large',\n",
    "         'axes.titlesize':'xx-large',\n",
    "         'xtick.labelsize':'xx-large',\n",
    "         'ytick.labelsize':'xx-large'}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "#append path with python modules\n",
    "import importlib\n",
    "import sys\n",
    "sys.path.append(\"/Users/jakubkusmierski/Desktop/Uczenie_Maszynowe_2/modules\")\n",
    "\n",
    "#Private functions\n",
    "import plotting_functions as plf\n",
    "importlib.reload(plf);\n",
    "\n",
    "import text_functions as txt_fcn\n",
    "importlib.reload(txt_fcn);\n",
    "\n",
    "#import text_functions as txt_fcn\n",
    "#importlib.reload(txt_fcn);\n",
    "#Hide GPU\n",
    "#tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "def map_fn(x):\n",
    "    middle = 2\n",
    "    features =  tf.concat((x[:,:middle], x[:,middle+1:]), axis=1)\n",
    "    label = x[:,middle]\n",
    "    return features, label\n",
    "###################################################\n",
    "def print_item(batch, vocabulary, width=2):\n",
    "    batch_index = 0\n",
    "    item = (batch[0][batch_index], batch[1][batch_index])\n",
    "    features = \" \".join(vocabulary[item[0].numpy()[0:width]])\n",
    "    label = vocabulary[item[1].numpy()]\n",
    "    print(colored(\"Features\", \"blue\"), end=\" \")\n",
    "    print(colored(\"(Label):\", \"red\"), end=\" \")\n",
    "\n",
    "    print(features, end=\" \")\n",
    "    print(colored(label,\"red\"), end=\" \")\n",
    "    features = \" \".join(vocabulary[item[0].numpy()[width:]])\n",
    "    print(features)\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mReading text from files: \u001b[0m ['/Users/jakubkusmierski/.keras/datasets/shakespeare.txt']\n",
      "\u001b[32mVocabulary length: \u001b[0m 12563\n",
      "\u001b[34mFeatures\u001b[0m \u001b[31m(Label):\u001b[0m before we \u001b[31mproceed\u001b[0m any further\n",
      "\u001b[34mFeatures: \u001b[0m [128  33 123 639] \u001b[34mLabel: \u001b[0m 1267\n",
      "\u001b[34mFeatures\u001b[0m \u001b[31m(Label):\u001b[0m are accounted \u001b[31mpoor\u001b[0m citizens the\n",
      "\u001b[34mFeatures: \u001b[0m [  40 7153 1196    2] \u001b[34mLabel: \u001b[0m 149\n",
      "\u001b[34mFeatures\u001b[0m \u001b[31m(Label):\u001b[0m is a \u001b[31mgain\u001b[0m to them\n",
      "\u001b[34mFeatures: \u001b[0m [12  7  4 62] \u001b[34mLabel: \u001b[0m 1109\n",
      "\u001b[34mFeatures\u001b[0m \u001b[31m(Label):\u001b[0m services he \u001b[31mhas\u001b[0m done for\n",
      "\u001b[34mFeatures: \u001b[0m [1501   22  146   14] \u001b[34mLabel: \u001b[0m 271\n",
      "\u001b[34mFeatures\u001b[0m \u001b[31m(Label):\u001b[0m was for \u001b[31mhis\u001b[0m country he\n",
      "\u001b[34mFeatures: \u001b[0m [ 51  14 600  22] \u001b[34mLabel: \u001b[0m 20\n"
     ]
    }
   ],
   "source": [
    "# load text\n",
    "filePath = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "dataset = txt_fcn.load_wksf_dataset(filePath)\n",
    "\n",
    "# adapt vextorization layer\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(max_tokens=1000, output_mode = \"int\")\n",
    "vectorize_layer.adapt(dataset.batch(128))\n",
    "\n",
    "# split lines into words\n",
    "dataset = dataset.map(tf_text.WhitespaceTokenizer().tokenize)\n",
    "\n",
    "# fix all tf_text.sliding_window function arguments except for the input data\n",
    "window_size = 5\n",
    "slidingWindowWithWidth = functools.partial(tf_text.sliding_window, width=window_size)\n",
    "\n",
    "# apply the sliding window to each line.\n",
    "# this will produce a tensor of shape (n, width) for each line,\n",
    "# where n in the number of groups of words with length width\n",
    "dataset = dataset.map(slidingWindowWithWidth)\n",
    "\n",
    "# remove empty lines\n",
    "dataset = dataset.filter(lambda x: tf.size(x) > 0)\n",
    "\n",
    "# split the (n, width) tensor into (n) tensors of shape (width)\n",
    "dataset = dataset.unbatch()\n",
    "\n",
    "# merge words into sentence framgents\n",
    "dataset = dataset.map(lambda x: tf.strings.reduce_join(x, separator=' '))\n",
    "\n",
    "#Vectorize\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(output_mode=\"int\")\n",
    "vectorize_layer.adapt(dataset.batch(1024))\n",
    "vocabulary = np.array(vectorize_layer.get_vocabulary())\n",
    "vocabulary_length = vocabulary.shape[0]\n",
    "dataset_vectorized = dataset.batch(1024).map(vectorize_layer, num_parallel_calls=tf.data.AUTOTUNE).unbatch()\n",
    "dataset_vectorized = dataset_vectorized.filter(lambda x: tf.math.count_nonzero(x==1, axis=0) < 2)\n",
    "print(colored(\"Vocabulary length: \", \"green\"), vocabulary_length)\n",
    "\n",
    "dataset_final = dataset_vectorized.batch(32).map(map_fn)\n",
    "\n",
    "for item in dataset_final.take(5):\n",
    "    print_item(item, vocabulary, width=2)\n",
    "    print(colored(\"Features: \", \"blue\"), item[0][0].numpy(), end=\" \")\n",
    "    print(colored(\"Label: \", \"blue\"), item[1][0].numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTokens = len(vocabulary_length)\n",
    "nDims = 128 \n",
    "embedding_layer = tf.keras.layers.Embedding(nTokens, nDims)\n",
    "\n",
    "# Embeding space exploration - words similar to \"man\"\n",
    "vocabulary_embedding = embedding_layer(tf.range(vocabulary_length))\n",
    "\n",
    "word = \"man\"\n",
    "\n",
    "\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
